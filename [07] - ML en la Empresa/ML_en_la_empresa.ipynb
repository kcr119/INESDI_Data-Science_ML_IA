{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnc3UOfdQY6GvENlbX4RJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B07%5D%20-%20ML%20en%20la%20Empresa/ML_en_la_empresa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Caso Pr√°ctico: Industrializar un modelo\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Informaci√≥n del M√≥dulo\n",
        "\n",
        "**Asignatura:** Data Analytics: Data Science, Machine Learning e Inteligencia Artificial  \n",
        "**M√°ster:** FP en Business Analytics e Inteligencia Artificial  \n",
        "**Profesores:** √Ålvaro L√≥pez Barber√°\n",
        "**Ejemplo Pr√°ctico:**  ML en la empresa\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Objetivo\n",
        "\n",
        "Desarrollar un modelo de lenguaje natural sobre Machine Learning, que nos conteste a nuestras preguntas."
      ],
      "metadata": {
        "id": "6TW047jpphEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üì¶ PASO 1: Instalar dependencias y Configuraci√≥n Inicial"
      ],
      "metadata": {
        "id": "-YLy5GZmp47H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn fastapi uvicorn pydantic joblib requests numpy nest-asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LgZoJy3qlxR",
        "outputId": "e50e2596-3469-4ba7-f7ca-e475d8308cfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.118.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')  # Descarga todo (tarda ~5 min)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdNzkCoSgawz",
        "outputId": "3d6e8390-ba98-4a81-9906-0931e0c20571"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3EyuHuSHgnS8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos de NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"‚úÖ Recursos descargados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2bxHxeFrhHC",
        "outputId": "d0d6bd95-9b5f-4a15-da1c-060a8f5c1c5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Recursos descargados\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìä PASO 2: Crear Base de Conocimiento\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "CONSTRUYENDO BASE DE CONOCIMIENTO\n",
        "‚úì Total de preguntas en la base: 45\n",
        "‚úì Vectorizando preguntas con TF-IDF...\n",
        "‚úì Vectorizaci√≥n completada\n",
        "üíæ Guardando chatbot en chatbot_data.pkl...\n",
        "‚úÖ Chatbot guardado exitosamente"
      ],
      "metadata": {
        "id": "NOmv4rAaqBf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üìä PASO 2: Crear Base de Conocimiento\n",
        "\n",
        "### Archivo: `01_crear_base_conocimiento.py`\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "PASO 1: CREAR BASE DE CONOCIMIENTO DEL CHATBOT\n",
        "Creamos una base de preguntas-respuestas sobre Machine Learning\n",
        "y la preparamos para calcular similitudes\n",
        "\"\"\"\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURACI√ìN INICIAL: Descargar todos los recursos de NLTK\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFIGURACI√ìN INICIAL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nDescargando recursos de NLTK (puede tardar unos segundos)...\")\n",
        "recursos_nltk = ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4']\n",
        "\n",
        "for recurso in recursos_nltk:\n",
        "    try:\n",
        "        nltk.download(recurso, quiet=True)\n",
        "        print(f\"  ‚úì {recurso} descargado\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö† Error descargando {recurso}: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Recursos de NLTK listos\\n\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class ChatbotMLFAQ:\n",
        "    \"\"\"\n",
        "    Chatbot simple basado en similitud de texto para FAQs de Machine Learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Base de conocimiento: preguntas y respuestas sobre ML\n",
        "        self.qa_pairs = [\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es machine learning?\",\n",
        "                    \"¬øQu√© es el aprendizaje autom√°tico?\",\n",
        "                    \"Define machine learning\",\n",
        "                    \"Explica qu√© es ML\"\n",
        "                ],\n",
        "                \"respuesta\": \"Machine Learning es una rama de la Inteligencia Artificial que permite a las m√°quinas aprender de los datos sin ser programadas expl√≠citamente. Los algoritmos identifican patrones y toman decisiones bas√°ndose en ejemplos anteriores.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es overfitting?\",\n",
        "                    \"¬øQu√© es el sobreajuste?\",\n",
        "                    \"Explica overfitting\",\n",
        "                    \"Qu√© significa sobreajustar\"\n",
        "                ],\n",
        "                \"respuesta\": \"Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido. Esto hace que funcione muy bien con los datos de entrenamiento pero mal con datos nuevos. Se soluciona con validaci√≥n cruzada, regularizaci√≥n o m√°s datos.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es una red neuronal?\",\n",
        "                    \"Define red neuronal\",\n",
        "                    \"Explica las redes neuronales\",\n",
        "                    \"Qu√© son las neural networks\"\n",
        "                ],\n",
        "                \"respuesta\": \"Una red neuronal es un modelo de aprendizaje profundo inspirado en el cerebro humano. Est√° compuesta por capas de neuronas artificiales conectadas que procesan informaci√≥n. Se usa para tareas complejas como reconocimiento de im√°genes o procesamiento de lenguaje natural.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© diferencia hay entre clasificaci√≥n y regresi√≥n?\",\n",
        "                    \"Clasificaci√≥n vs regresi√≥n\",\n",
        "                    \"Diferencia entre clasificaci√≥n y regresi√≥n\",\n",
        "                    \"Cu√°ndo usar clasificaci√≥n o regresi√≥n\"\n",
        "                ],\n",
        "                \"respuesta\": \"La clasificaci√≥n predice categor√≠as o clases (ej: spam/no spam, gato/perro), mientras que la regresi√≥n predice valores num√©ricos continuos (ej: precio de una casa, temperatura). Clasificaci√≥n da etiquetas, regresi√≥n da n√∫meros.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es el accuracy?\",\n",
        "                    \"Define accuracy\",\n",
        "                    \"Qu√© mide el accuracy\",\n",
        "                    \"Explica la m√©trica accuracy\"\n",
        "                ],\n",
        "                \"respuesta\": \"Accuracy (precisi√≥n) es el porcentaje de predicciones correctas sobre el total de predicciones. Se calcula como: (predicciones correctas / total de predicciones) √ó 100. Es √∫til cuando las clases est√°n balanceadas, pero puede enga√±ar con datos desbalanceados.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es gradient descent?\",\n",
        "                    \"Explica gradient descent\",\n",
        "                    \"Qu√© es el descenso de gradiente\",\n",
        "                    \"C√≥mo funciona gradient descent\"\n",
        "                ],\n",
        "                \"respuesta\": \"Gradient Descent es un algoritmo de optimizaci√≥n que ajusta los par√°metros de un modelo para minimizar el error. Funciona calculando el gradiente (derivada) de la funci√≥n de p√©rdida y movi√©ndose en la direcci√≥n opuesta para encontrar el m√≠nimo.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es cross-validation?\",\n",
        "                    \"Explica la validaci√≥n cruzada\",\n",
        "                    \"Para qu√© sirve cross-validation\",\n",
        "                    \"Qu√© es k-fold\"\n",
        "                ],\n",
        "                \"respuesta\": \"Cross-validation es una t√©cnica para evaluar modelos dividing los datos en k partes (folds). Se entrena k veces usando k-1 partes para entrenar y 1 para validar, rotando. Esto da una estimaci√≥n m√°s robusta del rendimiento real del modelo.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es un Random Forest?\",\n",
        "                    \"Explica Random Forest\",\n",
        "                    \"C√≥mo funciona Random Forest\",\n",
        "                    \"Qu√© es bosque aleatorio\"\n",
        "                ],\n",
        "                \"respuesta\": \"Random Forest es un modelo ensemble que combina m√∫ltiples √°rboles de decisi√≥n. Cada √°rbol se entrena con una muestra aleatoria de datos y features. La predicci√≥n final es el promedio (regresi√≥n) o voto mayoritario (clasificaci√≥n) de todos los √°rboles.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es feature engineering?\",\n",
        "                    \"Explica feature engineering\",\n",
        "                    \"Para qu√© sirve feature engineering\",\n",
        "                    \"Qu√© es ingenier√≠a de caracter√≠sticas\"\n",
        "                ],\n",
        "                \"respuesta\": \"Feature Engineering es el proceso de crear nuevas caracter√≠sticas (features) a partir de los datos existentes para mejorar el rendimiento del modelo. Incluye transformaciones, combinaciones, extracciones y selecci√≥n de las variables m√°s relevantes.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es supervised learning?\",\n",
        "                    \"Explica aprendizaje supervisado\",\n",
        "                    \"Qu√© es supervised learning\",\n",
        "                    \"Diferencia entre supervisado y no supervisado\"\n",
        "                ],\n",
        "                \"respuesta\": \"Supervised Learning es cuando entrenamos un modelo con datos etiquetados (sabemos la respuesta correcta). El modelo aprende la relaci√≥n entre inputs y outputs. Ejemplos: clasificaci√≥n de emails, predicci√≥n de precios. Se diferencia del no supervisado que trabaja sin etiquetas.\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"¬øQu√© es un hiperpar√°metro?\",\n",
        "                    \"Diferencia entre par√°metro e hiperpar√°metro\",\n",
        "                    \"Explica hiperpar√°metros\",\n",
        "                    \"Qu√© son los hyperparameters\"\n",
        "                ],\n",
        "                \"respuesta\": \"Los hiperpar√°metros son configuraciones del modelo que definimos ANTES del entrenamiento (ej: learning rate, n√∫mero de capas). Los par√°metros son los valores que el modelo APRENDE durante el entrenamiento (ej: pesos de una red neuronal).\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Hola\",\n",
        "                    \"Buenos d√≠as\",\n",
        "                    \"Buenas tardes\",\n",
        "                    \"Hey\"\n",
        "                ],\n",
        "                \"respuesta\": \"¬°Hola! Soy un chatbot especializado en Machine Learning. Puedo responder preguntas sobre conceptos de ML, algoritmos, m√©tricas y m√°s. ¬øEn qu√© puedo ayudarte?\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Gracias\",\n",
        "                    \"Muchas gracias\",\n",
        "                    \"Perfecto gracias\",\n",
        "                    \"Ok gracias\"\n",
        "                ],\n",
        "                \"respuesta\": \"¬°De nada! Si tienes m√°s preguntas sobre Machine Learning, estar√© encantado de ayudarte. üòä\"\n",
        "            },\n",
        "            {\n",
        "                \"preguntas\": [\n",
        "                    \"Adi√≥s\",\n",
        "                    \"Hasta luego\",\n",
        "                    \"Chao\",\n",
        "                    \"Nos vemos\"\n",
        "                ],\n",
        "                \"respuesta\": \"¬°Hasta pronto! Que tengas un buen d√≠a aprendiendo Machine Learning. üöÄ\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Preparar lematizador y stopwords\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "        # Vectorizador TF-IDF\n",
        "        self.vectorizer = None\n",
        "        self.question_vectors = None\n",
        "        self.all_questions = []\n",
        "        self.question_to_answer = {}\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto: tokeniza, elimina stopwords y lematiza\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convertir a min√∫sculas\n",
        "            text = text.lower()\n",
        "\n",
        "            # Tokenizar\n",
        "            tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "            # Eliminar stopwords y lematizar\n",
        "            processed_tokens = [\n",
        "                self.lemmatizer.lemmatize(token)\n",
        "                for token in tokens\n",
        "                if token.isalnum() and token not in self.stop_words\n",
        "            ]\n",
        "\n",
        "            return ' '.join(processed_tokens)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Si falla la tokenizaci√≥n en espa√±ol, intentar con ingl√©s\n",
        "            print(f\"‚ö† Advertencia en preprocesamiento: {e}\")\n",
        "            try:\n",
        "                tokens = word_tokenize(text)\n",
        "                processed_tokens = [\n",
        "                    self.lemmatizer.lemmatize(token)\n",
        "                    for token in tokens\n",
        "                    if token.isalnum()\n",
        "                ]\n",
        "                return ' '.join(processed_tokens)\n",
        "            except:\n",
        "                # √öltimo recurso: dividir por espacios\n",
        "                return ' '.join(text.lower().split())\n",
        "\n",
        "    def build_knowledge_base(self):\n",
        "        \"\"\"\n",
        "        Construye la base de conocimiento y vectoriza las preguntas\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"CONSTRUYENDO BASE DE CONOCIMIENTO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Crear lista de todas las preguntas y mapeo a respuestas\n",
        "        for qa_pair in self.qa_pairs:\n",
        "            respuesta = qa_pair[\"respuesta\"]\n",
        "            for pregunta in qa_pair[\"preguntas\"]:\n",
        "                pregunta_procesada = self.preprocess_text(pregunta)\n",
        "                self.all_questions.append(pregunta_procesada)\n",
        "                self.question_to_answer[pregunta_procesada] = respuesta\n",
        "\n",
        "        print(f\"\\n‚úì Total de preguntas en la base: {len(self.all_questions)}\")\n",
        "        print(f\"‚úì Total de respuestas √∫nicas: {len(self.qa_pairs)}\")\n",
        "\n",
        "        # Vectorizar preguntas usando TF-IDF\n",
        "        print(\"\\n Vectorizando preguntas con TF-IDF...\")\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.question_vectors = self.vectorizer.fit_transform(self.all_questions)\n",
        "\n",
        "        print(\"‚úì Vectorizaci√≥n completada\")\n",
        "\n",
        "    def get_response(self, user_question, threshold=0.3):\n",
        "        \"\"\"\n",
        "        Obtiene la respuesta m√°s similar a la pregunta del usuario\n",
        "\n",
        "        Args:\n",
        "            user_question: Pregunta del usuario\n",
        "            threshold: Umbral m√≠nimo de similitud (0-1)\n",
        "\n",
        "        Returns:\n",
        "            Respuesta del chatbot y score de confianza\n",
        "        \"\"\"\n",
        "        # Preprocesar pregunta del usuario\n",
        "        processed_question = self.preprocess_text(user_question)\n",
        "\n",
        "        # Vectorizar pregunta del usuario\n",
        "        user_vector = self.vectorizer.transform([processed_question])\n",
        "\n",
        "        # Calcular similitud con todas las preguntas\n",
        "        similarities = cosine_similarity(user_vector, self.question_vectors)[0]\n",
        "\n",
        "        # Encontrar la pregunta m√°s similar\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_similarity = similarities[best_match_idx]\n",
        "\n",
        "        # Si la similitud es muy baja, no sabemos la respuesta\n",
        "        if best_similarity < threshold:\n",
        "            return {\n",
        "                \"answer\": \"Lo siento, no tengo informaci√≥n sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, etc.\",\n",
        "                \"confidence\": float(best_similarity),\n",
        "                \"matched_question\": None\n",
        "            }\n",
        "\n",
        "        # Obtener la respuesta\n",
        "        matched_question = self.all_questions[best_match_idx]\n",
        "        answer = self.question_to_answer[matched_question]\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"confidence\": float(best_similarity),\n",
        "            \"matched_question\": matched_question\n",
        "        }\n",
        "\n",
        "    def save(self, filename=\"chatbot_data.pkl\"):\n",
        "        \"\"\"\n",
        "        Guarda el chatbot completo\n",
        "        \"\"\"\n",
        "        print(f\"\\nüíæ Guardando chatbot en {filename}...\")\n",
        "        joblib.dump({\n",
        "            'vectorizer': self.vectorizer,\n",
        "            'question_vectors': self.question_vectors,\n",
        "            'all_questions': self.all_questions,\n",
        "            'question_to_answer': self.question_to_answer,\n",
        "            'lemmatizer': self.lemmatizer,\n",
        "            'stop_words': self.stop_words\n",
        "        }, filename)\n",
        "        print(f\"‚úÖ Chatbot guardado exitosamente\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Funci√≥n principal para crear y guardar el chatbot\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"‚ñà\" * 60)\n",
        "    print(\" CREACI√ìN DE CHATBOT ML - BASE DE CONOCIMIENTO\")\n",
        "    print(\"‚ñà\" * 60)\n",
        "\n",
        "    # Crear chatbot\n",
        "    chatbot = ChatbotMLFAQ()\n",
        "\n",
        "    # Construir base de conocimiento\n",
        "    chatbot.build_knowledge_base()\n",
        "\n",
        "    # Guardar chatbot\n",
        "    chatbot.save()\n",
        "\n",
        "    # Prueba r√°pida\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PRUEBA R√ÅPIDA DEL CHATBOT\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    test_questions = [\n",
        "        \"¬øQu√© es machine learning?\",\n",
        "        \"Expl√≠came overfitting\",\n",
        "        \"¬øC√≥mo funciona gradient descent?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n‚ùì Pregunta: {question}\")\n",
        "        response = chatbot.get_response(question)\n",
        "        print(f\"ü§ñ Respuesta: {response['answer'][:100]}...\")\n",
        "        print(f\"üìä Confianza: {response['confidence']:.2%}\")\n",
        "\n",
        "    print(\"\\n\" + \"‚ñà\" * 60)\n",
        "    print(\" ‚úÖ CHATBOT CREADO Y GUARDADO EXITOSAMENTE\")\n",
        "    print(\"‚ñà\" * 60)\n",
        "    print(\"\\nAhora ejecuta: python 02_api_chatbot.py\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870367f4-8530-4761-b9a8-e3e34c25ff8f",
        "id": "54rbr9IWlyRc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CONFIGURACI√ìN INICIAL\n",
            "============================================================\n",
            "\n",
            "Descargando recursos de NLTK (puede tardar unos segundos)...\n",
            "  ‚úì punkt descargado\n",
            "  ‚úì punkt_tab descargado\n",
            "  ‚úì stopwords descargado\n",
            "  ‚úì wordnet descargado\n",
            "  ‚úì omw-1.4 descargado\n",
            "\n",
            "‚úÖ Recursos de NLTK listos\n",
            "\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " CREACI√ìN DE CHATBOT ML - BASE DE CONOCIMIENTO\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "============================================================\n",
            "CONSTRUYENDO BASE DE CONOCIMIENTO\n",
            "============================================================\n",
            "\n",
            "‚úì Total de preguntas en la base: 56\n",
            "‚úì Total de respuestas √∫nicas: 14\n",
            "\n",
            " Vectorizando preguntas con TF-IDF...\n",
            "‚úì Vectorizaci√≥n completada\n",
            "\n",
            "üíæ Guardando chatbot en chatbot_data.pkl...\n",
            "‚úÖ Chatbot guardado exitosamente\n",
            "\n",
            "============================================================\n",
            "PRUEBA R√ÅPIDA DEL CHATBOT\n",
            "============================================================\n",
            "\n",
            "‚ùì Pregunta: ¬øQu√© es machine learning?\n",
            "ü§ñ Respuesta: Machine Learning es una rama de la Inteligencia Artificial que permite a las m√°quinas aprender de lo...\n",
            "üìä Confianza: 100.00%\n",
            "\n",
            "‚ùì Pregunta: Expl√≠came overfitting\n",
            "ü§ñ Respuesta: Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el...\n",
            "üìä Confianza: 100.00%\n",
            "\n",
            "‚ùì Pregunta: ¬øC√≥mo funciona gradient descent?\n",
            "ü§ñ Respuesta: Gradient Descent es un algoritmo de optimizaci√≥n que ajusta los par√°metros de un modelo para minimiz...\n",
            "üìä Confianza: 85.50%\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " ‚úÖ CHATBOT CREADO Y GUARDADO EXITOSAMENTE\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "Ahora ejecuta: python 02_api_chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üåê PASO 3: API REST del Chatbot\n",
        "\n",
        "**Salida esperada:**\n",
        "```\n",
        "INICIANDO API DEL CHATBOT ML\n",
        "üìÇ Cargando chatbot desde chatbot_data.pkl...\n",
        "‚úÖ Chatbot cargado exitosamente\n",
        "   - Preguntas en base: 45\n",
        "\n",
        "Iniciando servidor en: http://127.0.0.1:8000\n",
        "Documentaci√≥n: http://127.0.0.1:8000/docs"
      ],
      "metadata": {
        "id": "snkxoUCErsJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üåê PASO 3: API REST del Chatbot\n",
        "\n",
        "### Archivo: `02_api_chatbot.py`\n",
        "\n",
        "\"\"\"\n",
        "PASO 2: API REST PARA EL CHATBOT\n",
        "Expone el chatbot a trav√©s de una API REST con FastAPI\n",
        "\"\"\"\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "import joblib\n",
        "from typing import Optional\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURACI√ìN DE LA API\n",
        "# ============================================================================\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"ML FAQ Chatbot API\",\n",
        "    description=\"API de chatbot para preguntas sobre Machine Learning\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Configurar CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Variable global para el chatbot\n",
        "chatbot_data = None\n",
        "\n",
        "# ============================================================================\n",
        "# MODELOS DE DATOS\n",
        "# ============================================================================\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    \"\"\"\n",
        "    Modelo de entrada: pregunta del usuario\n",
        "    \"\"\"\n",
        "    question: str = Field(\n",
        "        ...,\n",
        "        description=\"Pregunta del usuario sobre Machine Learning\",\n",
        "        min_length=1,\n",
        "        max_length=500\n",
        "    )\n",
        "    threshold: Optional[float] = Field(\n",
        "        0.3,\n",
        "        description=\"Umbral m√≠nimo de confianza (0-1)\",\n",
        "        ge=0.0,\n",
        "        le=1.0\n",
        "    )\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"question\": \"¬øQu√© es overfitting?\",\n",
        "                \"threshold\": 0.3\n",
        "            }\n",
        "        }\n",
        "\n",
        "class ChatResponse(BaseModel):\n",
        "    \"\"\"\n",
        "    Modelo de respuesta del chatbot\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    confidence: float\n",
        "    matched_question: Optional[str]\n",
        "    status: str\n",
        "\n",
        "# ============================================================================\n",
        "# CLASE CHATBOT (versi√≥n para API)\n",
        "# ============================================================================\n",
        "\n",
        "class ChatbotAPI:\n",
        "    \"\"\"\n",
        "    Versi√≥n del chatbot para usar en la API\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_file=\"chatbot_data.pkl\"):\n",
        "        \"\"\"\n",
        "        Carga el chatbot desde el archivo guardado\n",
        "        \"\"\"\n",
        "        if not os.path.exists(data_file):\n",
        "            raise FileNotFoundError(\n",
        "                f\"No se encontr√≥ {data_file}. \"\n",
        "                f\"Ejecuta primero: python 01_crear_base_conocimiento.py\"\n",
        "            )\n",
        "\n",
        "        print(f\"üìÇ Cargando chatbot desde {data_file}...\")\n",
        "        data = joblib.load(data_file)\n",
        "\n",
        "        self.vectorizer = data['vectorizer']\n",
        "        self.question_vectors = data['question_vectors']\n",
        "        self.all_questions = data['all_questions']\n",
        "        self.question_to_answer = data['question_to_answer']\n",
        "        self.lemmatizer = data['lemmatizer']\n",
        "        self.stop_words = data['stop_words']\n",
        "\n",
        "        print(\"‚úÖ Chatbot cargado exitosamente\")\n",
        "        print(f\"   - Preguntas en base: {len(self.all_questions)}\")\n",
        "        print(f\"   - Respuestas √∫nicas: {len(set(self.question_to_answer.values()))}\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesa el texto\n",
        "        \"\"\"\n",
        "        from nltk.tokenize import word_tokenize\n",
        "\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text, language='spanish')\n",
        "        processed_tokens = [\n",
        "            self.lemmatizer.lemmatize(token)\n",
        "            for token in tokens\n",
        "            if token.isalnum() and token not in self.stop_words\n",
        "        ]\n",
        "        return ' '.join(processed_tokens)\n",
        "\n",
        "    def get_response(self, user_question, threshold=0.3):\n",
        "        \"\"\"\n",
        "        Obtiene respuesta para la pregunta del usuario\n",
        "        \"\"\"\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        import numpy as np\n",
        "\n",
        "        # Preprocesar pregunta\n",
        "        processed_question = self.preprocess_text(user_question)\n",
        "\n",
        "        # Vectorizar\n",
        "        user_vector = self.vectorizer.transform([processed_question])\n",
        "\n",
        "        # Calcular similitud\n",
        "        similarities = cosine_similarity(user_vector, self.question_vectors)[0]\n",
        "\n",
        "        # Mejor coincidencia\n",
        "        best_match_idx = np.argmax(similarities)\n",
        "        best_similarity = similarities[best_match_idx]\n",
        "\n",
        "        # Verificar umbral\n",
        "        if best_similarity < threshold:\n",
        "            return {\n",
        "                \"answer\": \"Lo siento, no tengo informaci√≥n sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, Random Forest, etc.\",\n",
        "                \"confidence\": float(best_similarity),\n",
        "                \"matched_question\": None,\n",
        "                \"status\": \"low_confidence\"\n",
        "            }\n",
        "\n",
        "        # Obtener respuesta\n",
        "        matched_question = self.all_questions[best_match_idx]\n",
        "        answer = self.question_to_answer[matched_question]\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"confidence\": float(best_similarity),\n",
        "            \"matched_question\": matched_question,\n",
        "            \"status\": \"success\"\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# EVENTOS DE LA API\n",
        "# ============================================================================\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    \"\"\"\n",
        "    Carga el chatbot al iniciar la API\n",
        "    \"\"\"\n",
        "    global chatbot_data\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INICIANDO API DEL CHATBOT ML\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        chatbot_data = ChatbotAPI()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n‚ùå ERROR: {e}\\n\")\n",
        "        raise\n",
        "\n",
        "# ============================================================================\n",
        "# ENDPOINTS\n",
        "# ============================================================================\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"\n",
        "    Endpoint ra√≠z - informaci√≥n de la API\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"message\": \"ML FAQ Chatbot API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"description\": \"Chatbot especializado en preguntas sobre Machine Learning\",\n",
        "        \"endpoints\": {\n",
        "            \"chat\": \"/chat (POST)\",\n",
        "            \"health\": \"/health (GET)\",\n",
        "            \"stats\": \"/stats (GET)\",\n",
        "            \"examples\": \"/examples (GET)\",\n",
        "            \"docs\": \"/docs (GET)\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"\n",
        "    Verificaci√≥n de salud de la API\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"chatbot_loaded\": True,\n",
        "        \"questions_in_base\": len(chatbot_data.all_questions)\n",
        "    }\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "async def stats():\n",
        "    \"\"\"\n",
        "    Estad√≠sticas del chatbot\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    return {\n",
        "        \"total_questions\": len(chatbot_data.all_questions),\n",
        "        \"unique_answers\": len(set(chatbot_data.question_to_answer.values())),\n",
        "        \"topics\": [\n",
        "            \"Machine Learning b√°sico\",\n",
        "            \"Overfitting y validaci√≥n\",\n",
        "            \"Algoritmos (Random Forest, Gradient Descent)\",\n",
        "            \"M√©tricas (Accuracy)\",\n",
        "            \"Conceptos (Feature Engineering, Hiperpar√°metros)\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "@app.get(\"/examples\")\n",
        "async def examples():\n",
        "    \"\"\"\n",
        "    Ejemplos de preguntas que el chatbot puede responder\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"examples\": [\n",
        "            \"¬øQu√© es machine learning?\",\n",
        "            \"Expl√≠came qu√© es overfitting\",\n",
        "            \"¬øQu√© diferencia hay entre clasificaci√≥n y regresi√≥n?\",\n",
        "            \"¬øQu√© es gradient descent?\",\n",
        "            \"¬øPara qu√© sirve cross-validation?\",\n",
        "            \"¬øQu√© es Random Forest?\",\n",
        "            \"Explica feature engineering\",\n",
        "            \"¬øQu√© es un hiperpar√°metro?\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "@app.post(\"/chat\", response_model=ChatResponse)\n",
        "async def chat(request: ChatRequest):\n",
        "    \"\"\"\n",
        "    Endpoint principal del chatbot\n",
        "\n",
        "    Recibe una pregunta y devuelve la respuesta del chatbot\n",
        "    \"\"\"\n",
        "    if chatbot_data is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Chatbot no cargado\")\n",
        "\n",
        "    try:\n",
        "        # Obtener respuesta del chatbot\n",
        "        response = chatbot_data.get_response(\n",
        "            request.question,\n",
        "            threshold=request.threshold\n",
        "        )\n",
        "\n",
        "        return ChatResponse(\n",
        "            question=request.question,\n",
        "            answer=response[\"answer\"],\n",
        "            confidence=response[\"confidence\"],\n",
        "            matched_question=response[\"matched_question\"],\n",
        "            status=response[\"status\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Error al procesar la pregunta: {str(e)}\"\n",
        "        )\n",
        "\n",
        "# ============================================================================\n",
        "# EJECUTAR SERVIDOR\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SERVIDOR API - ML FAQ Chatbot\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nIniciando servidor en: http://127.0.0.1:8000\")\n",
        "    print(\"Documentaci√≥n: http://127.0.0.1:8000/docs\")\n",
        "    print(\"\\nPresiona CTRL+C para detener\\n\")\n",
        "\n",
        "    # Configuraci√≥n para evitar conflictos con event loops existentes\n",
        "    import sys\n",
        "\n",
        "    # Detectar si estamos en Jupyter/Colab\n",
        "    try:\n",
        "        get_ipython()\n",
        "        IN_NOTEBOOK = True\n",
        "    except NameError:\n",
        "        IN_NOTEBOOK = False\n",
        "\n",
        "    if IN_NOTEBOOK:\n",
        "        # En Jupyter/Colab, usar nest_asyncio\n",
        "        print(\"‚ö†Ô∏è  Detectado entorno Jupyter/Colab\")\n",
        "        print(\"   Ejecutando servidor en modo compatible...\\n\")\n",
        "\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "        except ImportError:\n",
        "            print(\"‚ùå Instalando nest_asyncio...\")\n",
        "            import subprocess\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nest_asyncio\"])\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "\n",
        "        # Iniciar servidor en segundo plano\n",
        "        import threading\n",
        "\n",
        "        def run_server():\n",
        "            uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "        server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "        server_thread.start()\n",
        "\n",
        "        print(\"‚úÖ Servidor iniciado en segundo plano\")\n",
        "        print(\"   Accede a: http://127.0.0.1:8000/docs\")\n",
        "        print(\"   El servidor se detendr√° cuando cierres el notebook\\n\")\n",
        "    else:\n",
        "        # En terminal normal\n",
        "        uvicorn.run(\n",
        "            app,\n",
        "            host=\"0.0.0.0\",\n",
        "            port=8000,\n",
        "            log_level=\"info\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e229a32-dbd8-47e4-f83d-4f579a55f26c",
        "id": "d_Fzo9JkmEnn"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4206241009.py:171: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SERVIDOR API - ML FAQ Chatbot\n",
            "============================================================\n",
            "\n",
            "Iniciando servidor en: http://127.0.0.1:8000\n",
            "Documentaci√≥n: http://127.0.0.1:8000/docs\n",
            "\n",
            "Presiona CTRL+C para detener\n",
            "\n",
            "‚ö†Ô∏è  Detectado entorno Jupyter/Colab\n",
            "   Ejecutando servidor en modo compatible...\n",
            "\n",
            "‚úÖ Servidor iniciado en segundo plano\n",
            "   Accede a: http://127.0.0.1:8000/docs\n",
            "   El servidor se detendr√° cuando cierres el notebook\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üß™ PASO 4: Cliente de Prueba"
      ],
      "metadata": {
        "id": "xmGYeJN7r5O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## üß™ PASO 4: Cliente de Prueba\n",
        "\n",
        "### Archivo: `03_test_chatbot.py`\n",
        "\"\"\"\n",
        "PASO 3: CLIENTE PARA PROBAR EL CHATBOT\n",
        "Script para probar la API del chatbot\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "BASE_URL = \"http://127.0.0.1:8000\"\n",
        "\n",
        "def print_response(title, response):\n",
        "    \"\"\"\n",
        "    Imprime la respuesta de manera bonita\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Status: {response.status_code}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(json.dumps(data, indent=2, ensure_ascii=False))\n",
        "    else:\n",
        "        print(f\"Error: {response.text}\")\n",
        "\n",
        "def test_health():\n",
        "    \"\"\"\n",
        "    Test del health check\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/health\")\n",
        "    print_response(\"TEST 1: Health Check\", response)\n",
        "\n",
        "def test_stats():\n",
        "    \"\"\"\n",
        "    Test de estad√≠sticas\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/stats\")\n",
        "    print_response(\"TEST 2: Estad√≠sticas del Chatbot\", response)\n",
        "\n",
        "def test_examples():\n",
        "    \"\"\"\n",
        "    Test de ejemplos\n",
        "    \"\"\"\n",
        "    response = requests.get(f\"{BASE_URL}/examples\")\n",
        "    print_response(\"TEST 3: Preguntas de Ejemplo\", response)\n",
        "\n",
        "def test_chat_questions():\n",
        "    \"\"\"\n",
        "    Test de preguntas al chatbot\n",
        "    \"\"\"\n",
        "    questions = [\n",
        "        \"¬øQu√© es machine learning?\",\n",
        "        \"Expl√≠came el overfitting por favor\",\n",
        "        \"¬øCu√°l es la diferencia entre clasificaci√≥n y regresi√≥n?\",\n",
        "        \"¬øQu√© es el gradient descent?\",\n",
        "        \"Hola chatbot\",\n",
        "        \"Gracias por la ayuda\",\n",
        "        \"¬øQu√© es la f√≠sica cu√°ntica?\"  # Esta NO deber√≠a saber\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  TEST 4.{i}: Pregunta al Chatbot\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"‚ùì Pregunta: {question}\")\n",
        "\n",
        "        response = requests.post(\n",
        "            f\"{BASE_URL}/chat\",\n",
        "            json={\"question\": question}\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"\\nü§ñ Respuesta:\")\n",
        "            print(f\"   {data['answer']}\")\n",
        "            print(f\"\\nüìä Confianza: {data['confidence']:.2%}\")\n",
        "            print(f\"üìå Estado: {data['status']}\")\n",
        "            if data['matched_question']:\n",
        "                print(f\"üîç Pregunta similar: {data['matched_question']}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error: {response.text}\")\n",
        "\n",
        "def run_all_tests():\n",
        "    \"\"\"\n",
        "    Ejecuta todos los tests\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"‚ñà\" * 70)\n",
        "    print(\"  SUITE DE TESTS - ML FAQ CHATBOT API\")\n",
        "    print(\"‚ñà\" * 70)\n",
        "\n",
        "    try:\n",
        "        test_health()\n",
        "        test_stats()\n",
        "        test_examples()\n",
        "        test_chat_questions()\n",
        "\n",
        "        print(\"\\n\" + \"‚ñà\" * 70)\n",
        "        print(\"  ‚úÖ TODOS LOS TESTS COMPLETADOS\")\n",
        "        print(\"‚ñà\" * 70 + \"\\n\")\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"\\n‚ùå ERROR: No se puede conectar a la API\")\n",
        "        print(\"Aseg√∫rate de que el servidor est√© corriendo:\")\n",
        "        print(\"   python 02_api_chatbot.py\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR: {e}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc8d47cb-ccd6-49d5-aad3-62d6c7cd4e74",
        "id": "iLwGXTrYmRei"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3110]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INICIANDO API DEL CHATBOT ML\n",
            "============================================================\n",
            "\n",
            "üìÇ Cargando chatbot desde chatbot_data.pkl...\n",
            "‚úÖ Chatbot cargado exitosamente\n",
            "   - Preguntas en base: 56\n",
            "   - Respuestas √∫nicas: 14\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "  SUITE DE TESTS - ML FAQ CHATBOT API\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "INFO:     127.0.0.1:35750 - \"GET /health HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 1: Health Check\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"status\": \"healthy\",\n",
            "  \"chatbot_loaded\": true,\n",
            "  \"questions_in_base\": 56\n",
            "}\n",
            "INFO:     127.0.0.1:35762 - \"GET /stats HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 2: Estad√≠sticas del Chatbot\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"total_questions\": 56,\n",
            "  \"unique_answers\": 14,\n",
            "  \"topics\": [\n",
            "    \"Machine Learning b√°sico\",\n",
            "    \"Overfitting y validaci√≥n\",\n",
            "    \"Algoritmos (Random Forest, Gradient Descent)\",\n",
            "    \"M√©tricas (Accuracy)\",\n",
            "    \"Conceptos (Feature Engineering, Hiperpar√°metros)\"\n",
            "  ]\n",
            "}\n",
            "INFO:     127.0.0.1:35768 - \"GET /examples HTTP/1.1\" 200 OK\n",
            "\n",
            "======================================================================\n",
            "  TEST 3: Preguntas de Ejemplo\n",
            "======================================================================\n",
            "Status: 200\n",
            "{\n",
            "  \"examples\": [\n",
            "    \"¬øQu√© es machine learning?\",\n",
            "    \"Expl√≠came qu√© es overfitting\",\n",
            "    \"¬øQu√© diferencia hay entre clasificaci√≥n y regresi√≥n?\",\n",
            "    \"¬øQu√© es gradient descent?\",\n",
            "    \"¬øPara qu√© sirve cross-validation?\",\n",
            "    \"¬øQu√© es Random Forest?\",\n",
            "    \"Explica feature engineering\",\n",
            "    \"¬øQu√© es un hiperpar√°metro?\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.1: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: ¬øQu√© es machine learning?\n",
            "INFO:     127.0.0.1:35774 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   Machine Learning es una rama de la Inteligencia Artificial que permite a las m√°quinas aprender de los datos sin ser programadas expl√≠citamente. Los algoritmos identifican patrones y toman decisiones bas√°ndose en ejemplos anteriores.\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: machine learning\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.2: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: Expl√≠came el overfitting por favor\n",
            "INFO:     127.0.0.1:35784 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   Overfitting ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido. Esto hace que funcione muy bien con los datos de entrenamiento pero mal con datos nuevos. Se soluciona con validaci√≥n cruzada, regularizaci√≥n o m√°s datos.\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: overfitting\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.3: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: ¬øCu√°l es la diferencia entre clasificaci√≥n y regresi√≥n?\n",
            "INFO:     127.0.0.1:35796 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   La clasificaci√≥n predice categor√≠as o clases (ej: spam/no spam, gato/perro), mientras que la regresi√≥n predice valores num√©ricos continuos (ej: precio de una casa, temperatura). Clasificaci√≥n da etiquetas, regresi√≥n da n√∫meros.\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: diferencia clasificaci√≥n regresi√≥n\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.4: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: ¬øQu√© es el gradient descent?\n",
            "INFO:     127.0.0.1:35802 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   Gradient Descent es un algoritmo de optimizaci√≥n que ajusta los par√°metros de un modelo para minimizar el error. Funciona calculando el gradiente (derivada) de la funci√≥n de p√©rdida y movi√©ndose en la direcci√≥n opuesta para encontrar el m√≠nimo.\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: gradient descent\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.5: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: Hola chatbot\n",
            "INFO:     127.0.0.1:35804 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   ¬°Hola! Soy un chatbot especializado en Machine Learning. Puedo responder preguntas sobre conceptos de ML, algoritmos, m√©tricas y m√°s. ¬øEn qu√© puedo ayudarte?\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: hola\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.6: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: Gracias por la ayuda\n",
            "INFO:     127.0.0.1:35814 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   ¬°De nada! Si tienes m√°s preguntas sobre Machine Learning, estar√© encantado de ayudarte. üòä\n",
            "\n",
            "üìä Confianza: 100.00%\n",
            "üìå Estado: success\n",
            "üîç Pregunta similar: gracias\n",
            "\n",
            "======================================================================\n",
            "  TEST 4.7: Pregunta al Chatbot\n",
            "======================================================================\n",
            "‚ùì Pregunta: ¬øQu√© es la f√≠sica cu√°ntica?\n",
            "INFO:     127.0.0.1:35822 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "\n",
            "ü§ñ Respuesta:\n",
            "   Lo siento, no tengo informaci√≥n sobre eso. Intenta preguntarme sobre conceptos de Machine Learning como overfitting, redes neuronales, gradient descent, Random Forest, etc.\n",
            "\n",
            "üìä Confianza: 0.00%\n",
            "üìå Estado: low_confidence\n",
            "\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "  ‚úÖ TODOS LOS TESTS COMPLETADOS\n",
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üß™ PASO 5: Lanzar el servidor en local"
      ],
      "metadata": {
        "id": "B1VSOAVqr_IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EXPONER SERVIDOR EN GOOGLE COLAB\n",
        "# ========================================\n",
        "\n",
        "# Usar el proxy de Colab\n",
        "from google.colab.output import eval_js\n",
        "print(\"üîÑ Obteniendo URL del servidor...\")\n",
        "\n",
        "# La URL base de Colab\n",
        "colab_url = eval_js(\"google.colab.kernel.proxyPort(8000)\")\n",
        "\n",
        "print(\"\\n\" + \"‚úÖ\" * 35)\n",
        "print(\"\\n   üéâ SERVIDOR ACTIVO EN COLAB üéâ\")\n",
        "print(\"\\n\" + \"‚úÖ\" * 35)\n",
        "print(f\"\\nüåê URL del servidor:\")\n",
        "print(f\"   {colab_url}\")\n",
        "print(f\"\\nüìñ ABRE ESTA URL EN TU NAVEGADOR:\")\n",
        "print(f\"   üëâ {colab_url}/docs\")\n",
        "print(f\"\\nüìç Otros endpoints:\")\n",
        "print(f\"   ‚Ä¢ Health:   {colab_url}/health\")\n",
        "print(f\"   ‚Ä¢ Chat:     {colab_url}/chat\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\\nüí° Click en la URL para abrir en nueva pesta√±a\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "E6na00WNmDo8",
        "outputId": "32350407-64c9-482d-fa77-5050cf188624"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Obteniendo URL del servidor...\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "   üéâ SERVIDOR ACTIVO EN COLAB üéâ\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
            "\n",
            "üåê URL del servidor:\n",
            "   https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev\n",
            "\n",
            "üìñ ABRE ESTA URL EN TU NAVEGADOR:\n",
            "   üëâ https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/docs\n",
            "\n",
            "üìç Otros endpoints:\n",
            "   ‚Ä¢ Health:   https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/health\n",
            "   ‚Ä¢ Chat:     https://8000-m-s-1b5it1befvjgq-d.us-east1-1.prod.colab.dev/chat\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üí° Click en la URL para abrir en nueva pesta√±a\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üí° C√ìMO FUNCIONA EL CHATBOT\n",
        "\n",
        "### 1. Procesamiento de Texto (NLP)\n",
        "\n",
        "```python\n",
        "Pregunta original: \"¬øQu√© es el overfitting?\"\n",
        "                    ‚Üì\n",
        "Preprocesamiento:  - Min√∫sculas\n",
        "                   - Tokenizaci√≥n\n",
        "                   - Eliminar stopwords (\"qu√©\", \"es\", \"el\")\n",
        "                   - Lematizaci√≥n\n",
        "                    ‚Üì\n",
        "Texto procesado:   \"overfitting\"\n",
        "```\n",
        "\n",
        "### 2. Vectorizaci√≥n TF-IDF\n",
        "\n",
        "```\n",
        "TF-IDF convierte texto en n√∫meros:\n",
        "\n",
        "\"overfitting\" ‚Üí [0.0, 0.0, 0.87, 0.0, 0.0, ...]\n",
        "                 (vector de 100+ dimensiones)\n",
        "```\n",
        "\n",
        "### 3. Similitud Coseno\n",
        "\n",
        "```python\n",
        "Pregunta usuario:    [0.0, 0.0, 0.87, ...]\n",
        "                            ‚Üì\n",
        "         Calcular similitud con TODAS las preguntas\n",
        "                            ‚Üì\n",
        "Pregunta 1: 0.23 ‚Üê\n",
        "Pregunta 2: 0.95 ‚Üê ¬°MEJOR MATCH!\n",
        "Pregunta 3: 0.15 ‚Üê\n",
        "                            ‚Üì\n",
        "            Devolver respuesta de Pregunta 2\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_Y2LKYissh4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üéì EJERCICIOS INTERESANTES\n",
        "\n",
        "### Ejercicio 1: A√±adir m√°s preguntas (F√°cil)\n",
        "Modifica `01_crear_base_conocimiento.py` para a√±adir 3 nuevas preguntas sobre Deep Learning.\n",
        "\n",
        "### Ejercicio 2: Ajustar el threshold (Medio)\n",
        "Experimenta cambiando el threshold en `/chat`. ¬øQu√© pasa si lo subes a 0.7? ¬øY si lo bajas a 0.1?\n",
        "\n",
        "### Ejercicio 3: Endpoint de feedback (Medio)\n",
        "Crea un nuevo endpoint `/feedback` que permita al usuario indicar si la respuesta fue √∫til.\n",
        "\n",
        "### Ejercicio 4: Historial de conversaci√≥n (Avanzado)\n",
        "Implementa un sistema que guarde el historial de preguntas en memoria para mantener contexto.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5gd74GLUssXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ CHECKLIST DE LA SESI√ìN\n",
        "\n",
        "- [ ] Entender qu√© es industrializaci√≥n de modelos\n",
        "- [ ] Conocer conceptos b√°sicos de NLP (tokenizaci√≥n, TF-IDF)\n",
        "- [ ] Comprender c√≥mo funciona similitud coseno\n",
        "- [ ] Crear una base de conocimiento\n",
        "- [ ] Serializar datos con pickle\n",
        "- [ ] Crear una API REST con FastAPI\n",
        "- [ ] Probar la API con diferentes m√©todos\n",
        "- [ ] Explorar documentaci√≥n autom√°tica (Swagger)\n",
        "- [ ] Modificar y extender el chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## üìö RECURSOS ADICIONALES\n",
        "\n",
        "### Documentaci√≥n:\n",
        "- **NLTK**: https://www.nltk.org/\n",
        "- **Scikit-learn TfidfVectorizer**: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "- **FastAPI**: https://fastapi.tiangolo.com/\n",
        "- **Cosine Similarity**: https://en.wikipedia.org/wiki/Cosine_similarity\n",
        "\n",
        "### Tutoriales:\n",
        "- Introduction to NLP with NLTK\n",
        "- Building REST APIs with FastAPI\n",
        "- Text Similarity with TF-IDF"
      ],
      "metadata": {
        "id": "BczROsC6tMVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéì PREGUNTAS FRECUENTES\n",
        "\n",
        "**P: ¬øPor qu√© usar TF-IDF y no simplemente contar palabras?**\n",
        "R: TF-IDF da m√°s peso a palabras importantes y menos a palabras comunes. \"Overfitting\" es m√°s importante que \"qu√©\" o \"es\".\n",
        "\n",
        "**P: ¬øEsto es un modelo de Machine Learning real?**\n",
        "R: No es un modelo que \"aprende\", pero usa t√©cnicas de ML (vectorizaci√≥n, similitud). Es perfecto para entender industrializaci√≥n.\n",
        "\n",
        "**P: ¬øPuedo usarlo para otros idiomas?**\n",
        "R: S√≠, solo cambia `language='spanish'` por `language='english'` en el c√≥digo y ajusta los stopwords.\n",
        "\n",
        "**P: ¬øC√≥mo lo escalo para miles de preguntas?**\n",
        "R: Considera usar Elasticsearch o FAISS para b√∫squedas vectoriales m√°s eficientes.\n",
        "\n",
        "**P: ¬øFunciona con WhatsApp/Telegram?**\n",
        "R: S√≠, puedes integrar la API con cualquier plataforma usando sus APIs (ej: Twilio para WhatsApp).\n"
      ],
      "metadata": {
        "id": "-aNoNxGTtWCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ MENSAJE FINAL\n",
        "\n",
        "Este proyecto demuestra que industrializar un modelo no siempre requiere infraestructura compleja. Con FastAPI + pickle + un poco de NLP, puedes crear un servicio √∫til en menos de 200 l√≠neas de c√≥digo.\n",
        "\n",
        "**Lo m√°s importante:** Los estudiantes ven el ciclo completo:\n",
        "1. Crear conocimiento ‚Üí 2. Guardarlo ‚Üí 3. Exponerlo como API ‚Üí 4. Consumirlo\n",
        "\n",
        "¬°Esto es industrializaci√≥n en su forma m√°s pr√°ctica!"
      ],
      "metadata": {
        "id": "rXYA0DXEtLi4"
      }
    }
  ]
}