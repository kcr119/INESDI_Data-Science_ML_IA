{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNylbbF9s4ZApgTZdrsG0oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/%5B05%5D_Evaluacion_y_optimizacion_sencillo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluaci√≥n y optimizaci√≥n"
      ],
      "metadata": {
        "id": "aqBtn1m5yWMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut-2FW2SxpHl",
        "outputId": "1fda3458-7c53-437d-a382-55805b7a7599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRID - Mejor combinaci√≥n: {'clf__C': 0.1, 'clf__penalty': 'l2'}\n",
            "GRID - Mejor F1 (CV):     0.986\n",
            "RANDOM - Mejor combinaci√≥n: {'clf__penalty': 'l2', 'clf__C': np.float64(0.11497569953977356)}\n",
            "RANDOM - Mejor F1 (CV):   0.988\n",
            "\n",
            "=== RESULTADOS EN TEST ===\n",
            "Matriz de confusi√≥n:\n",
            " [[40  2]\n",
            " [ 1 71]]\n",
            "Accuracy:    0.974\n",
            "Precision:   0.973\n",
            "Recall:      0.986\n",
            "F1-score:    0.979\n",
            "Specificity: 0.952\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# 0) IMPORTS Y DATOS (dataset binario sencillo)\n",
        "# =========================================================\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Datos reales y peque√±itos: benigno vs maligno (clasificaci√≥n binaria)\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# =========================================================\n",
        "# 1) SPLIT: TRAIN / VALIDACI√ìN+TEST\n",
        "#    (keep it simple: hacemos un holdout con test; la validaci√≥n la haremos v√≠a CV)\n",
        "# =========================================================\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "# - stratify mantiene proporciones de clases (estratificado)  ‚Üê recomendado con clases desbalanceadas\n",
        "#   (equivale a la \"stratified cross-validation\" de las diapositivas)  üìå\n",
        "\n",
        "# =========================================================\n",
        "# 2) PIPELINE BASE: ESCALADO + REGRESI√ìN LOG√çSTICA\n",
        "#    - Escalar ayuda a la regularizaci√≥n a \"tratar\" a todas las variables por igual.\n",
        "#    - Usamos solver \"saga\" para permitir L1 o L2.\n",
        "# =========================================================\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, solver=\"saga\"))\n",
        "])\n",
        "\n",
        "# =========================================================\n",
        "# 3) M√âTRICA OBJETIVO (elegimos F1 para equilibrar precision/recall)\n",
        "#    - Podr√≠as usar accuracy, roc_auc, etc. seg√∫n el problema.\n",
        "# =========================================================\n",
        "scoring = \"f1\"\n",
        "\n",
        "# =========================================================\n",
        "# 4) GRID SEARCH (prueba TODAS las combinaciones)\n",
        "#    - Hiperpar√°metros: tipo de regularizaci√≥n y su intensidad (C)\n",
        "#    - K-Fold estratificado para validar (cv=5)\n",
        "# =========================================================\n",
        "param_grid = {\n",
        "    \"clf__penalty\": [\"l1\", \"l2\"],      # L1 (lasso) = puede anular coeficientes; L2 (ridge) = encoge sin anular\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10, 100]  # C peque√±o => M√ÅS regularizaci√≥n; C grande => MENOS\n",
        "}\n",
        "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=scoring,\n",
        "    cv=cv5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"GRID - Mejor combinaci√≥n:\", grid.best_params_)\n",
        "print(f\"GRID - Mejor F1 (CV):     {grid.best_score_:.3f}\")\n",
        "\n",
        "# =========================================================\n",
        "# 5) RANDOM SEARCH (prueba combinaciones AL AZAR en un rango)\n",
        "#    - Suele encontrar buenas soluciones gastando menos tiempo que Grid.\n",
        "# =========================================================\n",
        "param_dist = {\n",
        "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
        "    \"clf__C\": np.logspace(-3, 3, 100)  # muestreamos C en un rango amplio (10^-3 a 10^3)\n",
        "}\n",
        "rand = RandomizedSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,                # n¬∫ de combinaciones aleatorias a probar (r√°pido)\n",
        "    scoring=scoring,\n",
        "    cv=cv5,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rand.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"RANDOM - Mejor combinaci√≥n:\", rand.best_params_)\n",
        "print(f\"RANDOM - Mejor F1 (CV):   {rand.best_score_:.3f}\")\n",
        "\n",
        "# =========================================================\n",
        "# 6) EVALUACI√ìN EN TEST (examen final)\n",
        "#    - ¬°Siempre eval√∫a el MEJOR modelo en datos NO vistos!\n",
        "# =========================================================\n",
        "best = grid if grid.best_score_ >= rand.best_score_ else rand\n",
        "y_pred = best.best_estimator_.predict(X_te)\n",
        "\n",
        "cm = confusion_matrix(y_te, y_pred)\n",
        "acc  = accuracy_score(y_te, y_pred)\n",
        "prec = precision_score(y_te, y_pred)\n",
        "rec  = recall_score(y_te, y_pred)\n",
        "f1   = f1_score(y_te, y_pred)\n",
        "spec = cm[0,0] / (cm[0,0] + cm[0,1])  # Specificity = TN / (TN + FP)\n",
        "\n",
        "print(\"\\n=== RESULTADOS EN TEST ===\")\n",
        "print(\"Matriz de confusi√≥n:\\n\", cm)\n",
        "print(f\"Accuracy:    {acc:.3f}\")\n",
        "print(f\"Precision:   {prec:.3f}\")\n",
        "print(f\"Recall:      {rec:.3f}\")\n",
        "print(f\"F1-score:    {f1:.3f}\")\n",
        "print(f\"Specificity: {spec:.3f}\")\n",
        "\n",
        "# =========================================================\n",
        "# 7) (OPCIONAL) EARLY STOPPING con XGBoost para ver el \"freno\" al overfitting\n",
        "#    - Requiere xgboost instalado. Si no lo tienes, comenta esta secci√≥n.\n",
        "#    - Se detiene cuando no mejora en el set de validaci√≥n durante 'n' rondas.\n",
        "# =========================================================\n",
        "# from xgboost import XGBClassifier\n",
        "# xgb = XGBClassifier(\n",
        "#     n_estimators=1000, learning_rate=0.05, max_depth=3, reg_lambda=1.0,\n",
        "#     eval_metric=\"logloss\", random_state=42\n",
        "# )\n",
        "# xgb.fit(\n",
        "#     X_tr, y_tr,\n",
        "#     eval_set=[(X_tr, y_tr), (X_te, y_te)],\n",
        "#     early_stopping_rounds=50,  # si no mejora en 50 rondas, se para\n",
        "#     verbose=False\n",
        "# )\n",
        "# print(\"\\nXGBoost - √Årboles realmente usados (best_iteration):\", xgb.best_iteration)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C√≥mo este ejemplo cubrimos ‚Äútodas las casu√≠sticas‚Äù del tema\n",
        "\n",
        "Hiperpar√°metros & Overfitting: regulas la complejidad con penalty (L1/L2) y C (intensidad). L1 puede ‚Äúapagar‚Äù variables; L2 suaviza pesos. Esto ataca el overfitting como ‚Äúfreno‚Äù del modelo.\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶\n",
        "\n",
        "Regularizaci√≥n: L1/L2 exactamente como en las diapositivas (analog√≠as lasso/ridge).\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶\n",
        "\n",
        "Validaci√≥n:\n",
        "\n",
        "Holdout (train/test) para examen final.\n",
        "\n",
        "K-Fold estratificado (cv=5) dentro de Grid/Random Search para seleccionar hiperpar√°metros sin fuga de informaci√≥n.\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶\n",
        "\n",
        "B√∫squeda de hiperpar√°metros:\n",
        "\n",
        "Grid Search (exhaustivo, m√°s lento, ‚Äúgarantiza‚Äù revisar la malla).\n",
        "\n",
        "Random Search (m√°s r√°pido, buena calidad en menos tiempo).\n",
        "\n",
        "(Mencionada la Bayesiana como tercera alternativa en clase).\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶\n",
        "\n",
        "M√©tricas y matriz de confusi√≥n: imprimimos accuracy, precision, recall, F1, specificity y la confusion matrix (TN, FP, FN, TP). √ösalas seg√∫n el coste de errores en tu caso.\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶\n",
        "\n",
        "Early stopping (opcional): con XGBoost mostramos el corte autom√°tico cuando deja de mejorar en validaci√≥n, evitando sobreentrenar.\n",
        "\n",
        "M√°ster de FP en BA e IA - Slide‚Ä¶"
      ],
      "metadata": {
        "id": "cVxqGQY2yPT_"
      }
    }
  ]
}