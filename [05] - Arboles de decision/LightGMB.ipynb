{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnMi8B+uv05yjh+Nptgg+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/INESDI_Data-Science_ML_IA/blob/main/%5B05%5D%20-%20Arboles%20de%20decision/LightGMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9eb426"
      },
      "source": [
        "# Arboles de decisi√≥n: LightGMB - Ejercicio 4: LightGMB.ipynb\n",
        "\n",
        "Este notebook es un **I do**: todo resuelto y explicado paso a paso."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos\n",
        "\n",
        "- Creo un dataset alearotio con 50000 ejemplos para predecir si un cliente comprar√°.\n",
        "- Entrena un LightGMB.\n",
        "- Eval√∫a el modelo con m√©tricas de clasificaci√≥n (accuracy, matriz de confusi√≥n y reporte).\n",
        "- Muestra la importancia de cada caracter√≠stica (qu√© variables usa m√°s el modelo para decidir).\n",
        "- Hacer una competici√≥n entre LightGMB VS XGBoost VS Gradient Boosting"
      ],
      "metadata": {
        "id": "SKZZUWm_pC3O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHcmvQLHYcbB"
      },
      "source": [
        "## 1) Instalamos y cargamos librerias xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ¬°IMPORTANTE! Instalar las librer√≠as:\n",
        "# pip install lightgbm xgboost\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "IM4RCzfMpc79"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJaQTnqpj_t"
      },
      "source": [
        "## 2) Preparamos datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparamos datos\n",
        "\n",
        "print(\"PREPARANDO EL CIRCUITO:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Dataset grande para ver diferencias reales\n",
        "np.random.seed(42)\n",
        "X_large, y_large = make_classification(\n",
        "    n_samples=50000,  # Dataset GRANDE para ver velocidad\n",
        "    n_features=100,\n",
        "    n_informative=80,\n",
        "    n_redundant=10,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# A√±adir caracter√≠sticas categ√≥ricas (ventaja para LightGBM)\n",
        "categorical_features = []\n",
        "for i in range(5):  # 5 caracter√≠sticas categ√≥ricas\n",
        "    cat_feature = np.random.choice(['A', 'B', 'C', 'D'], size=X_large.shape[0])\n",
        "    # Convertir a n√∫meros para sklearn\n",
        "    cat_encoded = pd.Categorical(cat_feature).codes\n",
        "    X_large = np.column_stack([X_large, cat_encoded])\n",
        "    categorical_features.append(X_large.shape[1] - 1)\n",
        "\n",
        "print(f\"üèÅ Circuito preparado:\")\n",
        "print(f\"   üìä Muestras: {X_large.shape[0]:,}\")\n",
        "print(f\"   üìã Caracter√≠sticas: {X_large.shape[1]} (5 categ√≥ricas)\")\n",
        "print(f\"   üè∑Ô∏è  Caracter√≠sticas categ√≥ricas: {categorical_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_8KCEqBpuPO",
        "outputId": "505b6979-c333-4960-dd31-1b7b051b0722"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPARANDO EL CIRCUITO:\n",
            "--------------------------------------------------\n",
            "üèÅ Circuito preparado:\n",
            "   üìä Muestras: 50,000\n",
            "   üìã Caracter√≠sticas: 105 (5 categ√≥ricas)\n",
            "   üè∑Ô∏è  Caracter√≠sticas categ√≥ricas: [100, 101, 102, 103, 104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear algunos valores faltantes\n",
        "n_missing = int(0.05 * X_large.shape[0] * X_large.shape[1])  # 5% faltantes\n",
        "missing_rows = np.random.choice(X_large.shape[0], size=n_missing, replace=True)\n",
        "missing_cols = np.random.choice(X_large.shape[1]-5, size=n_missing, replace=True)  # No en categ√≥ricas\n",
        "X_large[missing_rows, missing_cols] = np.nan\n",
        "\n",
        "print(f\"   üï≥Ô∏è  Valores faltantes: {np.isnan(X_large).sum():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9-vVASVp0a3",
        "outputId": "5909e2ff-c716-46c5-b323-7fe4dfff67d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üï≥Ô∏è  Valores faltantes: 255,703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_large, y_large, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"   üèãÔ∏è  Entrenamiento: {X_train.shape[0]:,} muestras\")\n",
        "print(f\"   üß™ Prueba: {X_test.shape[0]:,} muestras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIWf3Ydap0Pi",
        "outputId": "a4bdfb65-da41-4e55-ed51-743cddb16c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   üèãÔ∏è  Entrenamiento: 40,000 muestras\n",
            "   üß™ Prueba: 10,000 muestras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Preparamos los modelos"
      ],
      "metadata": {
        "id": "YFoOY58Gp5ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparar los modelos\n",
        "\n",
        "print(\"PREPARACI√ìN DE COMPETIDORES:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Configuraci√≥n com√∫n\n",
        "common_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuraci√≥n com√∫n:\")\n",
        "print(f\"   üå≥ Estimadores: {common_params['n_estimators']}\")\n",
        "print(f\"   üìè Profundidad: {common_params['max_depth']}\")\n",
        "print(f\"   üìö Learning rate: {common_params['learning_rate']}\")\n",
        "\n",
        "# Preparar datos para Gradient Boosting (necesita imputaci√≥n)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_filled = imputer.fit_transform(X_train)\n",
        "X_test_filled = imputer.transform(X_test)\n",
        "\n",
        "print(\"\\nüèéÔ∏è COMPETIDORES:\")\n",
        "\n",
        "# 1. Gradient Boosting Cl√°sico\n",
        "print(\"\\nü•â Competidor 1: Gradient Boosting Cl√°sico\")\n",
        "print(\"   üí™ Fortalezas: Estable, confiable, f√°cil\")\n",
        "print(\"   üò∞ Debilidades: Lento, necesita preprocesamiento\")\n",
        "\n",
        "gbm_model = GradientBoostingClassifier(**common_params)\n",
        "\n",
        "# 2. XGBoost\n",
        "print(\"\\nü•à Competidor 2: XGBoost\")\n",
        "print(\"   üí™ Fortalezas: Equilibrado, maduro, preciso\")\n",
        "print(\"   üò∞ Debilidades: M√°s lento que LightGBM\")\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    **common_params,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# 3. LightGBM\n",
        "print(\"\\nü•á Competidor 3: LightGBM\")\n",
        "print(\"   üí™ Fortalezas: S√öPER R√ÅPIDO, eficiente memoria\")\n",
        "print(\"   üò∞ Debilidades: Puede sobreajustar en datasets peque√±os\")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    **common_params,\n",
        "    verbose=-1  # Sin output detallado\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n9wA2I7p5uA",
        "outputId": "b260fed1-0fc1-4c20-cd2d-9524128fbf72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREPARACI√ìN DE COMPETIDORES:\n",
            "--------------------------------------------------\n",
            "‚öôÔ∏è  Configuraci√≥n com√∫n:\n",
            "   üå≥ Estimadores: 100\n",
            "   üìè Profundidad: 6\n",
            "   üìö Learning rate: 0.1\n",
            "\n",
            "üèéÔ∏è COMPETIDORES:\n",
            "\n",
            "ü•â Competidor 1: Gradient Boosting Cl√°sico\n",
            "   üí™ Fortalezas: Estable, confiable, f√°cil\n",
            "   üò∞ Debilidades: Lento, necesita preprocesamiento\n",
            "\n",
            "ü•à Competidor 2: XGBoost\n",
            "   üí™ Fortalezas: Equilibrado, maduro, preciso\n",
            "   üò∞ Debilidades: M√°s lento que LightGBM\n",
            "\n",
            "ü•á Competidor 3: LightGBM\n",
            "   üí™ Fortalezas: S√öPER R√ÅPIDO, eficiente memoria\n",
            "   üò∞ Debilidades: Puede sobreajustar en datasets peque√±os\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Entrenamos los modelos"
      ],
      "metadata": {
        "id": "_LFJZhl_rQnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#entrenamos los modelos\n",
        "\n",
        "print(\"üèÅ CARRERA DE VELOCIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Diccionario para almacenar resultados\n",
        "results = {}\n",
        "\n",
        "print(\"üö¶ ¬°Preparados, listos, YA!\")\n",
        "\n",
        "# CARRERA 1: Gradient Boosting\n",
        "print(\"\\nü•â Corriendo Gradient Boosting...\")\n",
        "start_time = time.time()\n",
        "gbm_model.fit(X_train_filled, y_train)\n",
        "gbm_time = time.time() - start_time\n",
        "gbm_pred = gbm_model.predict(X_test_filled)\n",
        "gbm_pred_proba = gbm_model.predict_proba(X_test_filled)[:, 1]\n",
        "gbm_accuracy = accuracy_score(y_test, gbm_pred)\n",
        "gbm_logloss = log_loss(y_test, gbm_pred_proba)\n",
        "\n",
        "results['GradientBoosting'] = {\n",
        "    'time': gbm_time,\n",
        "    'accuracy': gbm_accuracy,\n",
        "    'logloss': gbm_logloss\n",
        "}\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {gbm_time:.2f}s\")\n",
        "print(f\"   üéØ Precisi√≥n: {gbm_accuracy:.4f}\")\n",
        "\n",
        "# CARRERA 2: XGBoost\n",
        "print(\"\\nü•à Corriendo XGBoost...\")\n",
        "start_time = time.time()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start_time\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "xgb_logloss = log_loss(y_test, xgb_pred_proba)\n",
        "\n",
        "results['XGBoost'] = {\n",
        "    'time': xgb_time,\n",
        "    'accuracy': xgb_accuracy,\n",
        "    'logloss': xgb_logloss\n",
        "}\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {xgb_time:.2f}s\")\n",
        "print(f\"   üéØ Precisi√≥n: {xgb_accuracy:.4f}\")\n",
        "\n",
        "# CARRERA 3: LightGBM (SIN caracter√≠sticas categ√≥ricas primero)\n",
        "print(\"\\nü•á Corriendo LightGBM (sin optimizaci√≥n)...\")\n",
        "start_time = time.time()\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_time_basic = time.time() - start_time\n",
        "lgb_pred_basic = lgb_model.predict(X_test)\n",
        "lgb_pred_proba_basic = lgb_model.predict_proba(X_test)[:, 1]\n",
        "lgb_accuracy_basic = accuracy_score(y_test, lgb_pred_basic)\n",
        "lgb_logloss_basic = log_loss(y_test, lgb_pred_proba_basic)\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {lgb_time_basic:.2f}s\")\n",
        "print(f\"   üéØ Precisi√≥n: {lgb_accuracy_basic:.4f}\")\n",
        "\n",
        "# CARRERA 4: LightGBM OPTIMIZADO (con caracter√≠sticas categ√≥ricas)\n",
        "print(\"\\nüöÄ Corriendo LightGBM OPTIMIZADO...\")\n",
        "lgb_optimized = lgb.LGBMClassifier(\n",
        "    **common_params,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "lgb_optimized.fit(\n",
        "    X_train, y_train,\n",
        "    categorical_feature=categorical_features\n",
        ")\n",
        "lgb_time_opt = time.time() - start_time\n",
        "lgb_pred_opt = lgb_optimized.predict(X_test)\n",
        "lgb_pred_proba_opt = lgb_optimized.predict_proba(X_test)[:, 1]\n",
        "lgb_accuracy_opt = accuracy_score(y_test, lgb_pred_opt)\n",
        "lgb_logloss_opt = log_loss(y_test, lgb_pred_proba_opt)\n",
        "\n",
        "results['LightGBM_basic'] = {\n",
        "    'time': lgb_time_basic,\n",
        "    'accuracy': lgb_accuracy_basic,\n",
        "    'logloss': lgb_logloss_basic\n",
        "}\n",
        "\n",
        "results['LightGBM_optimized'] = {\n",
        "    'time': lgb_time_opt,\n",
        "    'accuracy': lgb_accuracy_opt,\n",
        "    'logloss': lgb_logloss_opt\n",
        "}\n",
        "\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {lgb_time_opt:.2f}s\")\n",
        "print(f\"   üéØ Precisi√≥n: {lgb_accuracy_opt:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo3h0zXfrYyq",
        "outputId": "0cca1803-ebc0-4119-ba52-fef8544c7bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÅ CARRERA DE VELOCIDAD:\n",
            "--------------------------------------------------\n",
            "üö¶ ¬°Preparados, listos, YA!\n",
            "\n",
            "ü•â Corriendo Gradient Boosting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Evaluamos la velocidad"
      ],
      "metadata": {
        "id": "jUsomicarkeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üèÜ PODIO DE VELOCIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Ordenar por velocidad\n",
        "speed_ranking = sorted(results.items(), key=lambda x: x[1]['time'])\n",
        "\n",
        "print(\"üèÅ RESULTADOS DE LA CARRERA:\")\n",
        "print(f\"{'Posici√≥n':<3} {'Competidor':<20} {'Tiempo':<10} {'Precisi√≥n':<10} {'Speedup':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "fastest_time = speed_ranking[0][1]['time']\n",
        "for i, (name, metrics) in enumerate(speed_ranking):\n",
        "    speedup = fastest_time / metrics['time']\n",
        "    medal = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"  \"\n",
        "    print(f\"{medal:<3} {name:<20} {metrics['time']:<8.2f}s {metrics['accuracy']:<8.4f} {speedup:<8.1f}x\")"
      ],
      "metadata": {
        "id": "Cdj2oUiZrk0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Analizamos el uso de memoria (recursos)"
      ],
      "metadata": {
        "id": "FItUcOkcrmNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä AN√ÅLISIS DE MEMORIA:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "print(\"üíæ USO DE MEMORIA (aproximado):\")\n",
        "print(\"   ü•â Gradient Boosting: ~Baseline MB\")\n",
        "print(\"   ü•à XGBoost: ~Baseline + 20-30% MB\")\n",
        "print(\"   ü•á LightGBM: ~Baseline - 30-50% MB\")\n",
        "print(\"\\n   üìù LightGBM es mucho m√°s eficiente en memoria!\")"
      ],
      "metadata": {
        "id": "fke0pIdFrmYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Evaluamos las caracteristicas especiales del LightGMB"
      ],
      "metadata": {
        "id": "X-HdAwfvrr_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CARACTER√çSTICAS ESPECIALES DE LIGHTGBM\n",
        "\n",
        "print(\"üåü CARACTER√çSTICAS ESPECIALES DE LIGHTGBM:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"üè∑Ô∏è MANEJO NATIVO DE CATEGOR√çAS:\")\n",
        "print(\"   ‚úÖ LightGBM maneja categor√≠as SIN one-hot encoding\")\n",
        "print(\"   ‚ùå XGBoost necesita preprocessing\")\n",
        "print(\"   ‚ùå Gradient Boosting necesita preprocessing\")\n",
        "\n",
        "print(f\"\\nüìä En nuestro ejemplo:\")\n",
        "print(f\"   üî¢ Caracter√≠sticas categ√≥ricas: {len(categorical_features)}\")\n",
        "print(f\"   üöÄ LightGBM las us√≥ directamente\")\n",
        "print(f\"   üîÑ Otros modelos las trataron como num√©ricas\")\n",
        "\n",
        "# Comparar importancia de caracter√≠sticas\n",
        "print(\"\\nüìà IMPORTANCIA DE CARACTER√çSTICAS:\")\n",
        "print(\"\\nü•á LightGBM top 5:\")\n",
        "lgb_importance = lgb_optimized.feature_importances_\n",
        "top_lgb = np.argsort(lgb_importance)[-5:][::-1]\n",
        "for i, idx in enumerate(top_lgb):\n",
        "    feature_type = \"Categ√≥rica\" if idx in categorical_features else \"Num√©rica\"\n",
        "    print(f\"   {i+1}. Feature_{idx:2d} ({feature_type}): {lgb_importance[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nü•à XGBoost top 5:\")\n",
        "xgb_importance = xgb_model.feature_importances_\n",
        "top_xgb = np.argsort(xgb_importance)[-5:][::-1]\n",
        "for i, idx in enumerate(top_xgb):\n",
        "    feature_type = \"Categ√≥rica\" if idx in categorical_features else \"Num√©rica\"\n",
        "    print(f\"   {i+1}. Feature_{idx:2d} ({feature_type}): {xgb_importance[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "Ks_h_6uBrsU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Probamos con un dataset m√°s grande"
      ],
      "metadata": {
        "id": "AAFWAioFr50k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ESCALABILIDAD - DATASET M√ÅS GRANDE\n",
        "print(\" üèãÔ∏è PRUEBA DE ESCALABILIDAD:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"üî¨ Creando dataset MASIVO para ver diferencias extremas...\")\n",
        "\n",
        "# Dataset m√°s grande\n",
        "X_massive, y_massive = make_classification(\n",
        "    n_samples=100000,  # ¬°100K muestras!\n",
        "    n_features=50,\n",
        "    n_informative=40,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_massive, X_test_massive, y_train_massive, y_test_massive = train_test_split(\n",
        "    X_massive, y_massive, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset masivo: {X_train_massive.shape[0]:,} muestras\")\n",
        "\n",
        "# Solo probar LightGBM vs XGBoost (GBM ser√≠a muy lento)\n",
        "print(\"\\n‚ö° CARRERA EXTREMA (solo LightGBM vs XGBoost):\")\n",
        "\n",
        "# XGBoost en dataset masivo\n",
        "print(\"ü•à XGBoost en dataset masivo...\")\n",
        "start_time = time.time()\n",
        "xgb_massive = xgb.XGBClassifier(n_estimators=50, max_depth=6, random_state=42, eval_metric='logloss')\n",
        "xgb_massive.fit(X_train_massive, y_train_massive)\n",
        "xgb_massive_time = time.time() - start_time\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {xgb_massive_time:.2f}s\")\n",
        "\n",
        "# LightGBM en dataset masivo\n",
        "print(\"ü•á LightGBM en dataset masivo...\")\n",
        "start_time = time.time()\n",
        "lgb_massive = lgb.LGBMClassifier(n_estimators=50, max_depth=6, random_state=42, verbose=-1)\n",
        "lgb_massive.fit(X_train_massive, y_train_massive)\n",
        "lgb_massive_time = time.time() - start_time\n",
        "print(f\"   ‚è±Ô∏è  Tiempo: {lgb_massive_time:.2f}s\")\n",
        "\n",
        "print(f\"\\nüöÄ EN DATASET MASIVO:\")\n",
        "print(f\"   LightGBM es {xgb_massive_time/lgb_massive_time:.1f}x M√ÅS R√ÅPIDO que XGBoost!\")"
      ],
      "metadata": {
        "id": "o7xXeWQKr3Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Resultados finales y comparativa entre modelos"
      ],
      "metadata": {
        "id": "a2jeggKZr6gI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_TzYcFXDd1b"
      },
      "outputs": [],
      "source": [
        "# VEREDICTO FINAL\n",
        "print(\"üèÜ VEREDICTO FINAL:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"üìä RESUMEN DE RENDIMIENTO:\")\n",
        "print(f\"{'Modelo':<20} {'Velocidad':<10} {'Precisi√≥n':<10} {'Memoria':<10}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'LightGBM':<20} {'üöÄüöÄüöÄ':<10} {'üéØüéØüéØ':<10} {'üíæüíæüíæ':<10}\")\n",
        "print(f\"{'XGBoost':<20} {'üöÄüöÄ':<10} {'üéØüéØüéØ':<10} {'üíæüíæ':<10}\")\n",
        "print(f\"{'GradientBoosting':<20} {'üöÄ':<10} {'üéØüéØ':<10} {'üíæ':<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10) Conclusion y Consejos, cuando usar cada uno\n",
        "\n",
        "\n",
        "ü•á USA LIGHTGBM CUANDO:\"\n",
        " *  ‚úÖ Dataset grande (>10,000 muestras)\"\n",
        " *  ‚úÖ Velocidad es cr√≠tica\"\n",
        " *  ‚úÖ Tienes caracter√≠sticas categ√≥ricas\"\n",
        " *  ‚úÖ Memoria limitada\"\n",
        " *  ‚úÖ Necesitas experimentar r√°pido\"\n",
        " *  ‚úÖ Aplicaciones en producci√≥n\"\n",
        "\n",
        "ü•à USA XGBOOST CUANDO:\"\n",
        " *  ‚úÖ Dataset mediano (1,000-100,000)\"\n",
        " *  ‚úÖ M√°xima estabilidad\"\n",
        " *  ‚úÖ Ecosistema maduro\"\n",
        " *  ‚úÖ Competencias Kaggle tradicionales\"\n",
        "\n",
        "ü•â USA GRADIENT BOOSTING CUANDO:\"\n",
        " *  ‚úÖ Aprendiendo conceptos\"\n",
        " *  ‚úÖ Dataset peque√±o (<1,000)\"\n",
        " *  ‚úÖ Simplicidad es clave\"\n",
        " *  ‚úÖ Prototipado r√°pido\"\n",
        "\n",
        "üèÜ GANADOR ABSOLUTO:\"\n",
        " *  üëë LightGBM - El nuevo rey de Gradient Boosting\"\n",
        " *  üöÄ M√°s r√°pido, eficiente y preciso\"\n",
        " *  üéØ Perfecto para la era de Big Data\"\n",
        "\n",
        "üìö PROGRESI√ìN RECOMENDADA:\"\n",
        " *  1Ô∏è‚É£ Aprende con Gradient Boosting cl√°sico\"\n",
        " *  2Ô∏è‚É£ Domina XGBoost para estabilidad\")\n",
        " *  3Ô∏è‚É£ Migra a LightGBM para m√°ximo rendimiento\""
      ],
      "metadata": {
        "id": "EScmfGpRoXKE"
      }
    }
  ]
}